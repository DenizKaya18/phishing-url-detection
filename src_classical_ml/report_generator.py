# -*- coding: utf-8 -*-
"""
Report Generation Module
"""

import os
import shutil
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


class ReportGenerator:
    """Generates reports and visualizations"""
    
    def __init__(self, config):
        self.config = config
    
    def safe_copy_to_drive(self, local_path, fname=None, max_retries=3):
        """Safely copy file to output folder with retries"""
        try:
            if not fname:
                fname = os.path.basename(local_path)
            dst = os.path.join(self.config.output_folder, fname)
            
            if not os.path.exists(self.config.output_folder):
                try:
                    os.makedirs(self.config.output_folder, exist_ok=True)
                except Exception as e:
                    print(f"[WARNING] Failed to create output folder: {e}")
                    return False
            
            for attempt in range(1, max_retries + 1):
                try:
                    shutil.copy2(local_path, dst)
                    print(f"[OK] {fname} backed up to output: {dst}")
                    return True
                except Exception as e:
                    if attempt == max_retries:
                        print(f"[WARNING] Failed to copy {fname} to output: {e}")
                        return False
        except Exception as e:
            print(f"[WARNING] Error in safe_copy_to_drive: {e}")
            return False
    
    def save_report(self, df, filename, copy_to_drive=True):
        """Save report locally and optionally to output folder"""
        try:
            tmp = filename + ".tmp"
            df.to_csv(tmp, index=False)
            try:
                os.replace(tmp, filename)
            except Exception:
                shutil.move(tmp, filename)
            print(f"[OK] Saved locally: {filename}")
        except Exception as e:
            print(f"[ERROR] Failed to save {filename}: {e}")
            return False
        
        if copy_to_drive:
            return self.safe_copy_to_drive(filename, os.path.basename(filename))
        return True
    
    def generate_reports(self, full_report_data):
        """Generate all reports from training results"""
        print("\n" + "="*80)
        print("=== GENERATING REPORTS ===")
        print("="*80)
        
        df_report = pd.DataFrame(full_report_data)
        
        if df_report.empty:
            print("[WARNING] No data to generate reports")
            return
        
        # Ensure timing columns exist
        time_cols = ['Avg_Single_Pred_Time_ms', 'Std_Single_Pred_Time_ms']
        for c in time_cols:
            if c not in df_report.columns:
                df_report[c] = 0.0
            df_report[c] = pd.to_numeric(df_report[c], errors='coerce').fillna(0.0)
        
        # Round timing columns
        df_report['Avg_Single_Pred_Time_ms'] = df_report['Avg_Single_Pred_Time_ms'].round(6)
        df_report['Std_Single_Pred_Time_ms'] = df_report['Std_Single_Pred_Time_ms'].round(6)
        
        # 1. Detailed report per fold
        self.save_report(df_report, "Detailed_Performance_Report_Per_Fold.csv")
        
        # 2. Baseline average performance
        df_base = df_report[df_report['Scenario'] == 'BASELINE']
        if not df_base.empty:
            metrics_agg = [
                'Train_Time_Sec', 'Avg_Single_Pred_Time_ms', 'Std_Single_Pred_Time_ms',
                'Imbalance_Ratio', 'Accuracy', 'F1_Score', 'Precision', 'Recall',
                'Sensitivity', 'Specificity', 'FNR', 'FPR', 'TN', 'FP', 'FN', 'TP'
            ]
            valid_metrics = [c for c in metrics_agg if c in df_base.columns]
            
            if valid_metrics:
                avg_base = df_base.groupby('Model')[valid_metrics].mean().reset_index()
                num_cols = avg_base.select_dtypes(include=[np.number]).columns.tolist()
                avg_base[num_cols] = avg_base[num_cols].round(6)
                self.save_report(avg_base, "Baseline_Average_Performance.csv")
            
            # 3. Confusion matrices
            self._generate_confusion_matrices(df_base)
        
        # 4. Statistical analysis
        self._generate_statistical_report(df_base)
        
        print(f"\n=== REPORTS COMPLETED. Files saved to: {self.config.output_folder} ===")
    
    def _generate_confusion_matrices(self, df_base):
        """Generate confusion matrix visualizations"""
        cm_cols = ['TN', 'FP', 'FN', 'TP']
        if not all(c in df_base.columns for c in cm_cols):
            print("[WARNING] Confusion matrix columns missing, skipping CM plots")
            return
        
        avg_cm = df_base.groupby('Model')[cm_cols].mean()
        
        for model_name in avg_cm.index:
            try:
                tn, fp, fn, tp = avg_cm.loc[model_name].astype(float).values
                cm_arr = np.array([[tn, fp], [fn, tp]])
                
                plt.figure(figsize=(5, 4))
                sns.heatmap(
                    cm_arr, annot=True, fmt=".1f", cbar=False,
                    xticklabels=['Pred: Normal', 'Pred: Phishing'],
                    yticklabels=['True: Normal', 'True: Phishing']
                )
                plt.title(f"Avg CM - {model_name} (Baseline)")
                plt.ylabel("True")
                plt.xlabel("Pred")
                plt.tight_layout()
                
                img_name = f"CM_Baseline_{model_name.replace(' ', '_')}.png"
                plt.savefig(img_name, dpi=300)
                plt.close()
                
                self.safe_copy_to_drive(img_name, img_name)
            except Exception as e:
                print(f"[ERROR] Failed to generate CM for {model_name}: {e}")
    
    def _generate_statistical_report(self, df_base):
        """Generate comprehensive statistical significance report - Friedman + Nemenyi focus"""
        if df_base.empty:
            print("[WARNING] No baseline data for statistical analysis")
            return
        
        try:
            # Gerekli kÃ¼tÃ¼phaneler
            import numpy as np
            import pandas as pd
            from scipy.stats import friedmanchisquare, chi2
            import scikit_posthocs as sp
            
            # SonuÃ§larÄ± model bazÄ±nda topla
            results_per_model = {}
            for model in df_base['Model'].unique():
                model_data = df_base[df_base['Model'] == model].sort_values('Fold')
                accuracies = model_data['Accuracy'].values.tolist()
                
                results_per_model[model] = {
                    'fold_accuracies': accuracies,
                }
            
            model_names = list(results_per_model.keys())
            if not model_names:
                print("[WARNING] No models found for statistical analysis")
                return
            
            n_folds = len(results_per_model[model_names[0]]['fold_accuracies'])
            if any(len(results_per_model[m]['fold_accuracies']) != n_folds for m in model_names):
                print("[ERROR] Models have different number of folds â†’ analysis aborted")
                return
            
            report_lines = []
            report_lines.append("=" * 80)
            report_lines.append("ðŸ“Š COMPREHENSIVE STATISTICAL SIGNIFICANCE REPORT")
            report_lines.append("Friedman Test + Nemenyi Post-hoc - Classical ML Models")
            report_lines.append("=" * 80)
            report_lines.append(f"Folds: {n_folds} | Models: {len(model_names)}")
            report_lines.append("")
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 1. Friedman Testi
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_lines.append("=" * 80)
            report_lines.append("ðŸ”¬ FRIEDMAN TEST (Non-parametric ANOVA alternative)")
            report_lines.append("=" * 80)
            report_lines.append("H0: All models perform equally (no difference in ranks)")
            report_lines.append("")
            
            # Fold Ã— Model matrisi (her satÄ±r bir fold)
            data_matrix = np.array([results_per_model[m]['fold_accuracies'] for m in model_names]).T
            
            # Ortalama doÄŸruluklarÄ± da gÃ¶sterelim
            mean_acc = {m: np.mean(results_per_model[m]['fold_accuracies']) for m in model_names}
            report_lines.append("Model Performans Ã–zeti (Mean Accuracy):")
            for m, acc in sorted(mean_acc.items(), key=lambda x: x[1], reverse=True):
                report_lines.append(f"  {m:<25} {acc:.4f}")
            report_lines.append("")
            
            stat, p = friedmanchisquare(*data_matrix.T)
            
            report_lines.append(f"Chi-square statistic: {stat:.4f}")
            report_lines.append(f"p-value:              {p:.6f}")
            
            sig_marker = "***" if p < 0.01 else "**" if p < 0.05 else ""
            result = f"Significant{sig_marker} (reject H0)" if p < 0.05 else "Not significant (fail to reject H0)"
            report_lines.append(f"Result (Î±=0.05):      {result}")
            report_lines.append("")
            report_lines.append("Interpretation:")
            report_lines.append("  â€¢ p < 0.05 â†’ At least one model differs significantly")
            report_lines.append("  â€¢ p â‰¥ 0.05 â†’ No evidence of overall difference")
            report_lines.append("")
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 2. Nemenyi Post-hoc (sadece Friedman anlamlÄ±ysa)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            nemenyi_matrix = None
            if p < 0.05:
                report_lines.append("=" * 80)
                report_lines.append("ðŸ” NEMENYI POST-HOC TEST (Pairwise comparisons)")
                report_lines.append("=" * 80)
                report_lines.append("Performed only if Friedman p < 0.05")
                report_lines.append("")
                
                nemenyi = sp.posthoc_nemenyi_friedman(data_matrix)
                nemenyi.index = model_names
                nemenyi.columns = model_names
                
                # Matrisi rapor iÃ§in gÃ¼zel formatla
                report_lines.append("Nemenyi p-values (symmetric matrix):")
                report_lines.append(nemenyi.round(6).to_string())
                report_lines.append("")
                
                # AnlamlÄ± Ã§iftleri listele
                significant_pairs = []
                for i in range(len(model_names)):
                    for j in range(i+1, len(model_names)):
                        p_val = nemenyi.iloc[i, j]
                        if p_val < 0.05:
                            sig_marker = "***" if p_val < 0.01 else "**"
                            significant_pairs.append(
                                f"{model_names[i]} vs {model_names[j]}: p={p_val:.4f}{sig_marker}"
                            )
                
                if significant_pairs:
                    report_lines.append("Significant pairwise differences (p < 0.05):")
                    for pair in significant_pairs:
                        report_lines.append(f"  â€¢ {pair}")
                else:
                    report_lines.append("No significant pairwise differences found (all p â‰¥ 0.05)")
                report_lines.append("")
                
                nemenyi_matrix = nemenyi
            else:
                report_lines.append("Friedman testi anlamlÄ± deÄŸil â†’ Nemenyi post-hoc yapÄ±lmadÄ±.")
                report_lines.append("")
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 3. Ã–zet Tablo
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_lines.append("=" * 80)
            report_lines.append("ðŸ“‹ TEST SUMMARY & RECOMMENDATIONS")
            report_lines.append("=" * 80)
            report_lines.append("")
            
            summary_lines = [
                f"{'Test':<20} {'Purpose':<45} {'Result':<20}",
                "-" * 85,
                f"{'Friedman':<20} {'Overall difference among models':<45} {result:<20}",
            ]
            
            if p < 0.05:
                count_sig = len(significant_pairs) if 'significant_pairs' in locals() else 0
                summary_lines.append(
                    f"{'Nemenyi post-hoc':<20} {'Which pairs differ':<45} {count_sig} significant pairs"
                )
            else:
                summary_lines.append(
                    f"{'Nemenyi post-hoc':<20} {'Which pairs differ':<45} {'Not performed':<20}"
                )
            
            report_lines.extend(summary_lines)
            report_lines.append("")
            report_lines.append("ðŸ’¡ Practical Interpretation:")
            report_lines.append("  â€¢ Friedman â†’ global ranking testi (tÃ¼m modeller aynÄ± anda)")
            report_lines.append("  â€¢ Nemenyi â†’ hangi modellerin birbirinden gerÃ§ekten farklÄ± olduÄŸunu sÃ¶yler")
            report_lines.append("  â€¢ EÄŸer Nemenyi'de anlamlÄ± Ã§ift yoksa â†’ modeller pratikte Ã§ok yakÄ±n performans gÃ¶steriyor")
            report_lines.append("")
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Raporu kaydet & yazdÄ±r
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_text = "\n".join(report_lines)
            stat_path = "Friedman_Nemenyi_Statistical_Report.txt"
            
            with open(stat_path, "w", encoding="utf-8") as f:
                f.write(report_text)
            
            print(f"[OK] Friedman + Nemenyi statistical report saved: {stat_path}")
            print("\n" + report_text)
            
            # EÄŸer sÄ±nÄ±fÄ±nda safe_copy_to_drive varsa
            if hasattr(self, 'safe_copy_to_drive'):
                self.safe_copy_to_drive(stat_path, stat_path)
                
        except ImportError as ie:
            print(f"[ERROR] Missing library: {ie}")
            print("  â†’ Make sure 'scikit-posthocs' is installed: pip install scikit-posthocs")
        except Exception as e:
            print(f"[ERROR] Statistical analysis failed: {e}")
            import traceback
            traceback.print_exc()
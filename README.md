# URL Phishing Detection - Ensemble Deep Learning Framework

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.19.0-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

A robust, production-ready ensemble deep learning framework for detecting phishing URLs using advanced neural network architectures with comprehensive statistical validation.

âœ¨ Key Features

- Ensemble Learning

- CNNâ€“BiLSTM base model

- Multi-scale CNN architecture

- Attention-based neural model

- Wide (high-capacity) architecture

- Feature Engineering

- Character-level URL tokenization

- Vectorized numerical URL features

- Fold-isolated feature extraction (no data leakage)

- Evaluation & Validation

- Stratified K-fold cross-validation (default: 10-fold)

- Accuracy, Precision, Recall, F1-score, AUC-ROC

- Statistical significance testing (McNemar, t-test, Wilcoxon)

- Research-Oriented Design

- Fully modular codebase

- Deterministic training via fixed random seeds

- Clear separation of data processing, modeling, and evaluation



## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/DenizKaya18/phishing-url-detection.git](https://github.com/DenizKaya18/phishing-url-detection.git)
    cd url-phishing-detection
    ```

2.  **Create and activate a virtual environment (Optional but recommended):**
    ```bash
    # Windows
    python -m venv venv
    .\venv\Scripts\activate

    # Linux/Mac
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```


## âš¡ Quick Start

Running the Full Pipeline

To run the complete pipeline (Preprocessing â†’ Cross-Validation â†’ Final Training â†’ Evaluation) with GPU optimization:

python -m src.main

### Programmatic Usage

You can import modules to run specific parts of the pipeline manually:

```
from src.preprocessing import prepare_data_from_raw
from src.ensemble_classifier import OptimizedEnsembleURLClassifierCV

# 1. Load and Prepare Data
# Note: prepare_data_from_raw returns 8 values
(X_url_train, y_train, X_url_test, y_test, rows_all, 
 tokenizer, max_len, vocab_size) = prepare_data_from_raw("data/dataset.txt", test_size=0.2)

# Create row tuples required for feature extraction
rows_train = [(X_url_train[i], y_train[i]) for i in range(len(X_url_train))]
rows_test = [(X_url_test[i], y_test[i]) for i in range(len(X_url_test))]

# 2. Initialize Ensemble
classifier = OptimizedEnsembleURLClassifierCV(
    n_models=4,
    n_folds=10,
    random_seeds=[42, 123, 456, 789]
)

# 3. Cross-Validation
# Supports checkpointing and detailed metrics
classifier.cross_validate_ensemble(
    X_url_train, y_train, rows_train,
    epochs=15,
    batch_size=512
)

# 4. Final Training on Training Split
classifier.train_final_ensemble(
    X_url_train, y_train, rows_train,
    X_url_test, y_test, rows_test,
    epochs=15,
    batch_size=512
)

```

## ğŸ“ Project Structure

```
url-phishing-detection/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ ensemble_classifier.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ statistical_tests.py
â”‚	â”œâ”€â”€ feature_extraction.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ README.md        ğŸ‘ˆ dataset description and source
â”‚   â””â”€â”€ dataset.txt
â”‚       
â”‚
â”œâ”€â”€ cv_checkpoints/    # Auto-saved CV states
â”œâ”€â”€ models/   		   # Saved models (auto-generated)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

```

### Dataset

This study uses a publicly available phishing URL dataset from Mendeley Data:

Source: https://data.mendeley.com/datasets/vfszbj9b36/1

Original labels: legitimate, phishing

Encoded labels:

0 â†’ legitimate

1 â†’ phishing

Label encoding was performed without altering class semantics or sample distribution.

Further details are provided in data/README.md.


## ğŸ—ï¸ Model Architectures

### 1. Base Model (CNN-BiLSTM)
- **Embedding**: 64-dimensional character embeddings
- **CNN**: 64 filters, kernel size 3, L2 regularization
- **BiLSTM**: 32 units per direction, dropout 0.3
- **Dense**: Two layers (64â†’32 units) with batch normalization

### 2. Multi-CNN Model
- Multiple convolutional branches (kernel sizes: 3, 5)
- Parallel feature extraction at different scales
- BiLSTM for sequence modeling
- Feature concatenation and fusion

### 3. Attention Model
- Custom attention mechanism for character importance
- CNN for local pattern detection
- Attention-weighted feature aggregation
- Dense classification layers

### 4. Wide Model
- Increased capacity (64 CNN filters, 64 BiLSTM units)
- Enhanced feature representation
- Suitable for complex pattern recognition
- Balanced regularization

## ğŸ” Feature Extraction

- Character-level URL sequences (deep models)

- Vectorized numerical features (URL structure-based)

- Fold-isolated feature computation to prevent information leakage

- Standardized scaling applied only on training folds


## ğŸ“ˆ Statistical Tests

### Implemented Tests

1. **McNemar's Test**
   - Compares paired predictions
   - Tests for significant differences between models

2. **Paired t-Test**
   - Compares mean performance across folds
   - Parametric test for normally distributed data

3. **Wilcoxon Signed-Rank Test**
   - Non-parametric alternative to t-test
   - Robust to non-normal distributions


## âš™ï¸ Configuration

### Model Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `n_models` | 4 | Number of models in ensemble |
| `n_folds` | 10 | Cross-validation folds |
| `epochs` | 15 | Training epochs per model |
| `batch_size` | 512 | Training batch size |
| `learning_rate` | 0.008 | Adam optimizer learning rate |
| `embedding_dim` | 64 | Character embedding dimension |

### ğŸš€ Training Optimizations

The framework automatically handles advanced training configurations:

* **Mixed Precision Training:** Uses `mixed_float16` policy for faster training and lower memory usage on supported GPUs.
* **Dynamic Loss Scaling:** Ensures numerical stability during half-precision training.
* **Built-in Callbacks:**
    * `EarlyStopping`: Monitors validation loss (patience=5) to prevent overfitting.
    * `ReduceLROnPlateau`: Reduces learning rate (factor=0.3, patience=2) when convergence stalls.


## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.






